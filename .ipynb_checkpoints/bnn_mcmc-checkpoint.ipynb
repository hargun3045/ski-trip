{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Viz. \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# sns.set_palette(palette='deep')\n",
    "# sns_c = sns.color_palette(palette='deep')\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"backprop.csv\")                  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df.iloc[:,0]\n",
    "y_data = df.iloc[:,1]\n",
    "plt.figure(figsize=(4,6))\n",
    "plt.scatter(X_data, y_data,color='g' ,s = 20, alpha = 0.5, label='sample data')\n",
    "plt.title('Advertising Dataset'); \n",
    "plt.xlabel('TV',fontsize=14); \n",
    "plt.ylabel('Sales',fontsize=14)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline neural network regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFP method\n",
    "\n",
    "### Convert data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "tf.random.set_seed(42)\n",
    "# Set tensor numeric type.\n",
    "dtype = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(X_data, dtype=dtype)\n",
    "x = tf.reshape(x,(-1,1))\n",
    "y = tf.convert_to_tensor(y_data, dtype=dtype)\n",
    "y = tf.reshape(y, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Bayesian random variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input x\n",
    "# Hidden layer with two activations\n",
    "# Output with bias\n",
    "# Total weights = 5\n",
    "def basic_nn(w0,w1,b1):\n",
    "    h1 = tf.matmul(x,w0)\n",
    "    a1 = tf.math.sin(h1)\n",
    "    h2 = tf.matmul(a1,w1) + b1\n",
    "    y = tf.math.sin(h2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jds_ab = tfd.JointDistributionNamedAutoBatched(dict(\n",
    "\n",
    "    bias=tfd.Normal(\n",
    "        loc=[tf.cast(0.0, dtype)], \n",
    "        scale=[tf.cast(1.0, dtype)]\n",
    "    ),\n",
    "    \n",
    "    weights0 = tfd.Normal(\n",
    "    loc=[[tf.cast(0.0, dtype),tf.cast(0.0,dtype)]], \n",
    "    scale=[[tf.cast(1.0, dtype),tf.cast(10.0,dtype)]]\n",
    "    ),\n",
    "\n",
    "    weights1=tfd.Normal(\n",
    "    loc=[[tf.cast(0.0, dtype)], [tf.cast(0.0, dtype)]], \n",
    "    scale=[[tf.cast(1.0, dtype)], [tf.cast(10.0, dtype)]]\n",
    "    ),\n",
    "    \n",
    " \n",
    "\n",
    "    y= lambda weights0,weights1,bias: \n",
    "        tfd.Normal(\n",
    "            loc=basic_nn(weights0,weights1,bias), \n",
    "            scale=1\n",
    "        ) \n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob_fn(weights0=None, weights1=None,bias=None):\n",
    "    return jds_ab.log_prob(weights0=weights0,weights1=weights1, bias=bias, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of each chain.\n",
    "num_results = int(3.5e4)\n",
    "# Burn-in steps.`\n",
    "num_burnin_steps = int(1.5e4)\n",
    "\n",
    "# Hamiltonian Monte Carlo transition kernel. \n",
    "# In TFP a TransitionKernel returns a new state given some old state.\n",
    "hcm_kernel  = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "  target_log_prob_fn=target_log_prob_fn,\n",
    "  step_size=1.0,\n",
    "  num_leapfrog_steps=3\n",
    "  \n",
    ")\n",
    "\n",
    "# This adapts the inner kernel's step_size.\n",
    "adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "  inner_kernel = hcm_kernel,\n",
    "  num_adaptation_steps=int(num_burnin_steps * 0.8)\n",
    ")\n",
    "\n",
    "# Run the chain (with burn-in).\n",
    "@tf.function\n",
    "def run_chain():\n",
    "  # Run the chain (with burn-in). \n",
    "  # Implements MCMC via repeated TransitionKernel steps.\n",
    "  samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "      num_results=num_results,\n",
    "      num_burnin_steps=num_burnin_steps,\n",
    "      current_state=[\n",
    "          tf.convert_to_tensor([[1.0,1.0]], dtype=dtype),\n",
    "          tf.convert_to_tensor([[1.0],[1.0]], dtype=dtype),\n",
    "          tf.convert_to_tensor([1.0], dtype=dtype), \n",
    "      ],\n",
    "      kernel=adaptive_hmc,\n",
    "      trace_fn=lambda _, pkr: pkr.inner_results.is_accepted\n",
    "    )\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of chains. \n",
    "num_chains = 5\n",
    "# Run sampling. \n",
    "chains = [run_chain() for i in range(num_chains)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the betas\n",
    "chains_t = list(map(list, zip(*chains)))\n",
    "\n",
    "# Combining all samples \n",
    "chains_samples = [tf.squeeze(tf.concat(samples, axis=0)) for samples in chains_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas dataframe with the betas\n",
    "chains_df = pd.concat(\n",
    "    objs=[pd.DataFrame(samples.numpy()) for samples in chains_samples], \n",
    "    axis=1\n",
    ")\n",
    "params = ['w11','w12','w21','w22','b2']\n",
    "chains_df.columns = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample_id (which is 0-10500)\n",
    "# Then creating chain_sample_id(rolling from 0-35000)\n",
    "# Then creating chain_id (c_1 or c_2 or c_3)\n",
    "chains_df = chains_df \\\n",
    "    .assign(\n",
    "        sample_id=lambda x: range(x.shape[0]), \n",
    "        chain_sample_id=lambda x: x['sample_id'] % num_results,\n",
    "        chain_id=lambda x: (x['sample_id'] / num_results).astype(int) + 1\n",
    "    ) \\\n",
    "    .assign(chain_id=lambda x: 'c_' + x['chain_id'].astype(str)) \\\n",
    "    \n",
    "chains_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=len(params), ncols=2, figsize=(10, 8), constrained_layout=True)\n",
    "\n",
    "# Plotting for each parameter\n",
    "for i, param in enumerate(params):\n",
    "    sns.histplot(x=param, data=chains_df, hue='chain_id', kde=True, ax=axes[i][0])\n",
    "    sns.lineplot(x='chain_sample_id', y=param, data=chains_df, hue='chain_id', alpha=0.3, legend=False, ax=axes[i][1])\n",
    "\n",
    "fig.suptitle('Posterior Samples per Chain', y=1.03);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
