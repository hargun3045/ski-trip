{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Oky2mXp_t1jZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZdL7eKx2Riq4"
   },
   "outputs": [],
   "source": [
    "# GET THE MNIST DATA\n",
    "(x_train, y_test), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to \n",
    "# 1. Change dimensions\n",
    "# 2. Change datatype\n",
    "def binary_preprocess(imageset):\n",
    "    imageset = imageset.reshape(imageset.shape[0],28,28,1)/255.\n",
    "    return np.where(imageset > .5, 1.0,0.0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processed images \n",
    "x_train_images = binary_preprocess(x_train)\n",
    "x_test_images = binary_preprocess(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset object to get a mini-batch\n",
    "batch_size = 100\n",
    "train_size = x_train_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder():\n",
    "  # THE ENCODER DEFINITION\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(2), scale=1.), \n",
    "                            reinterpreted_batch_ndims=1)\n",
    "    input_shape = (28,28,1)\n",
    "    encoded = tf.keras.models.Sequential(\n",
    "          [\n",
    "            tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(256,activation='relu'),\n",
    "            tf.keras.layers.Dense(tfp.layers.IndependentNormal.params_size(2), \n",
    "                                         activation=None, name='z_params'),          \n",
    "            # No activation\n",
    "            tfp.layers.IndependentNormal(2, \n",
    "                    convert_to_tensor_fn=tfd.Distribution.sample, \n",
    "                    activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior, weight=1), \n",
    "                    name='z_layer'),\n",
    "        ]\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Sequential API decoder\n",
    "\n",
    "def get_decoder():\n",
    "    decoded = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(2,)),              \n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),            \n",
    "            tf.keras.layers.Dense(784),\n",
    "            tfp.layers.IndependentBernoulli((28,28,1), name='x_layer')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return decoded\n",
    "\n",
    "def autoencoder(encoder,decoder):\n",
    "    x_input = tf.keras.Input(shape=(28,28,1))\n",
    "    encoder = get_encoder()\n",
    "    decoder = get_decoder()\n",
    "    z = encoder(x_input)\n",
    "\n",
    "    # compile VAE model\n",
    "    model = tf.keras.Model(inputs=x_input, outputs=decoder(z))\n",
    "    model.compile(loss=negative_log_likelihood, \n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "    return model\n",
    "\n",
    "# the negative of log-likelihood for probabilistic output\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder():\n",
    "    # define prior distribution for the code, which is an isotropic Gaussian\n",
    "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(2), scale=1.), \n",
    "                            reinterpreted_batch_ndims=1)\n",
    "    # build layers argument for tf.keras.Sequential()\n",
    "    input_shape = (28,28,1)\n",
    "    layers = [tf.keras.layers.InputLayer(input_shape=input_shape)]\n",
    "    layers.append(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2,2), \n",
    "                              padding='valid', activation='relu'))\n",
    "    layers.append(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2,2), \n",
    "                              padding='valid', activation='relu'))\n",
    "    layers.append(tf.keras.layers.Flatten())\n",
    "    # the following two lines set the output to be a probabilistic distribution\n",
    "    layers.append(tf.keras.layers.Dense(tfp.layers.IndependentNormal.params_size(2), \n",
    "                             activation=None, name='z_params'))\n",
    "    layers.append(tfp.layers.IndependentNormal(2, \n",
    "        convert_to_tensor_fn=tfd.Distribution.sample, \n",
    "        activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior, weight=1), \n",
    "        name='z_layer'))\n",
    "    return tf.keras.Sequential(layers, name='encoder')\n",
    "\n",
    "\n",
    "def get_decoder():\n",
    "    layers = [tf.keras.layers.InputLayer(input_shape=2)]\n",
    "    layers.append(tf.keras.layers.Dense(7*7*32, activation=None))\n",
    "    layers.append(tf.keras.layers.Reshape((7,7,32)))\n",
    "    layers.append(tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, \n",
    "                                       padding='same', activation='relu'))\n",
    "    layers.append(tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, \n",
    "                                       padding='same', activation='relu'))\n",
    "    layers.append(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \n",
    "                                       padding='same'))\n",
    "    layers.append(tf.keras.layers.Flatten(name='x_params'))\n",
    "    # note that here we don't need \n",
    "    # `tf.keras.layers.Dense(tfp.layers.IndependentBernoulli.params_size(self.dim_x))` because \n",
    "    # we've restored the desired input shape with the last Conv2DTranspose layer\n",
    "    layers.append(tfp.layers.IndependentBernoulli((28,28,1), name='x_layer'))\n",
    "    return tf.keras.Sequential(layers, name='decoder')\n",
    "\n",
    "\n",
    "def autoencoder(encoder,decoder):\n",
    "    x_input = tf.keras.Input(shape=(28,28,1))\n",
    "    encoder = get_encoder()\n",
    "    decoder = get_decoder()\n",
    "    z = encoder(x_input)\n",
    "\n",
    "    # compile VAE model\n",
    "    model = tf.keras.Model(inputs=x_input, outputs=decoder(z))\n",
    "    model.compile(loss=negative_log_likelihood, \n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "    return model\n",
    "\n",
    "# the negative of log-likelihood for probabilistic output\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate \n",
    "harencoder = get_encoder()\n",
    "hardecoder = get_decoder()\n",
    "model = autoencoder(harencoder,hardecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 3s 3ms/step - loss: 247.7511\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 173.5452\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 168.6867\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 165.8147\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 163.8519\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 161.8502\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 160.0112\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 158.6994\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 157.5321\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 156.6127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x151e70970>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_images,x_train_images,batch_size=100,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify if there is some learning\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "test_sample = 10 \n",
    "\n",
    "# Prediction \n",
    "pred = model(x_test_images[test_sample:test_sample+1]).mode()\n",
    "pred = pred.numpy().squeeze()\n",
    "\n",
    "ax[0].imshow(x_test_images[test_sample].squeeze(),cmap='gray')\n",
    "ax[1].imshow(pred,cmap='gray')\n",
    "ax[0].set_title('True image',fontsize=14)\n",
    "ax[1].set_title('Reconstruction',fontsize=14);\n",
    "ax[0].axis('off');\n",
    "ax[1].axis('off');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
